_BASE_: ../DVIS_Plus/DVIS_DAQ/configs/dvis_daq/ytvis21/vit_adapter/DAQ_Offline_VitAdapterL.yaml  # file to inherit settings from

MODEL:
  META_ARCHITECTURE: "DVIS_DAQ_offline"   # offline mode tracks all frames at once
  BACKBONE:
    NAME: "D2VitAdapterDinoV2"  # DINOv2 pre-trained ViT adapter as backbone
  VIT_ADAPTER:
    FINETUNE: True  # choose to fine tune the adapter
    NAME: 'vitl'
    VIT_WEIGHT: '/home/simone/checkpoints/dinov2_vitl14_pretrain.pth'   # path to pre-trained weights
    FREEZE_VIT: True  # FIXED: Must be True when using FINETUNE with selective unfreezing
    FINETUNE_INDEXES: [2, 3]   # Fine-tuning the last two stages (12 blocks) for better adaptation.
  SEM_SEG_HEAD:
    NUM_CLASSES: 5 
  MASK_FORMER:
    # Loss weights optimized for better classification and tracking
    NO_OBJECT_WEIGHT: 0.1      # Weight for no-object classification
    CLASS_WEIGHT: 4.0          # INCREASED: Weight for classification loss (was 2.0)
    MASK_WEIGHT: 3.0           # REDUCED: Weight for mask segmentation loss (was 5.0)
    DICE_WEIGHT: 3.0           # REDUCED: Weight for dice loss (was 5.0)
    TEST:
      TASK: 'vis'  # video instance segmentation task
      WINDOW_INFERENCE: True  # run inference on small frame windows for memory savings
      WINDOW_SIZE: 31  # size of the window for inference (smaller = less memory)
  VIDEO_HEAD:
    NUM_NEW_INS: 10  # maximum number of new instances to consider per frame
    TRAINING_SELECT_THRESHOLD: 0.02
    INFERENCE_SELECT_THRESHOLD: 0.01
    AUX_INFERENCE_SELECT_THRESHOLD: 0.01
    OFFLINE_TOPK_NUM: 40
    USE_LOCAL_ATTN: True

DATASETS:
  DATASET_RATIO: [1.0] # Ratio for mixing multiple datasets (1.0 means use all of this dataset)
  DATASET_NEED_MAP: [False]  # whether the dataset needs category mapping (False = no mapping needed)
  DATASET_TYPE: ['video_instance']  # type of dataset (video_instance = video instance segmentation)
  TRAIN: ("ytvis_fishway_train",)  # training dataset
  TEST: ("ytvis_fishway_val",)  # testing dataset

SOLVER:
  IMS_PER_BATCH: 1  # Reduced batch size for higher resolution training
  BASE_LR: 0.00005  # REDUCED: More stable learning rate for classification (was 0.0001)
  STEPS: (2424, 3232)     # ADJUSTED: Iterations at which the learning rate is decreased (60% and 80% of 3680)
  MAX_ITER: 4040     # INCREASED: Maximum number of training iterations (30 epochs - was 20 epochs)
  WARMUP_ITERS: 202   # INCREASED: Number of iterations for learning rate warmup (5% of 3680)
  CHECKPOINT_PERIOD: 202  # How often (in iterations) to save checkpoints (every epoch)
  BACKBONE_MULTIPLIER: 0.3  # ADJUSTED: Higher multiplier for backbone fine-tuning (was 0.1)
  # Memory optimization settings
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "norm"
    CLIP_VALUE: 0.1
    NORM_TYPE: 2.0

INPUT:
  # Higher resolution training with enhanced size variations for better generalization
  MIN_SIZE_TRAIN: (288, 320, 352, 384, 416, 448, 480, 512)  # range or lengths of the shorter side of the images
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"  # Sample different sizes per clip
  MAX_SIZE_TRAIN: 512  # max length of the longer side of the images (aspect ratio is preserved)
  
  # Testing sizes
  MIN_SIZE_TEST: 288 # minimum length of the shorter side of the images
  MAX_SIZE_TEST: 896 # maximum length of the shorter side of the images
  
  # Frame sampling
  SAMPLING_FRAME_NUM: 31 # number of frames to sample from each video clip during training
  SAMPLING_FRAME_RANGE: 15 # how many frames to sample before and after the reference frame
  SAMPLING_FRAME_STRIDE: 2 # 1 is every frame, 2 is every other frame, etc.
  
  # MOTION-PRESERVING AUGMENTATIONS ONLY
  # REMOVED: REVERSE_AGU - This would destroy temporal motion patterns
  RANDOM_FLIP: "flip_by_clip"  # Flip all frames in a clip the same way
  FORMAT: "RGB"  # Color format of the images
  
  # Crop augmentation for better generalization
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (480, 640)  # Increased crop size for higher resolution
  
  # Large Scale Jittering for robust training
  LSJ_AUG:
    ENABLED: True
    IMAGE_SIZE: 512  # Increased LSJ image size
    MIN_SCALE: 0.5
    MAX_SCALE: 1.0
  
  # Supported augmentations (motion-preserving)
  AUGMENTATIONS: ['brightness', 'contrast', 'saturation']  # Motion-preserving color augmentations
  # Note: 'rotation' removed as it would distort motion trajectories

TEST:
  AUG:
    ENABLED: false
    FLIP: true
    MAX_SIZE: 512
    MIN_SIZES:
    - 288
    - 320
    - 352
    - 384
    - 416
    - 448
    - 480
    - 512
DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: True # whether to filter out images with no annotations

OUTPUT_DIR: '/store/simone/dvis-model-outputs/trained_models/model_camera_s2'