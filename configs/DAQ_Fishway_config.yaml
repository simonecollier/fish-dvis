_BASE_: ../DVIS_Plus/DVIS_DAQ/configs/dvis_daq/ytvis21/vit_adapter/DAQ_Offline_VitAdapterL.yaml  # file to inherit settings from

MODEL:
  META_ARCHITECTURE: "DVIS_DAQ_offline"   # offline mode tracks all frames at once
  BACKBONE:
    NAME: "D2VitAdapterDinoV2"  # DINOv2 pre-trained ViT adapter as backbone
  VIT_ADAPTER:
    FINETUNE: True  # choose to fine tune the adapter
    NAME: 'vitl'
    VIT_WEIGHT: '/home/simone/checkpoints/dinov2_vitl14_pretrain.pth'   # path to pre-trained weights
    FREEZE_VIT: True  # choose to freeze the main ViT backbone (trains only the adapter)
    FINETUNE_INDEXES: [2, 3]   # Fine-tuning the last two stages (12 blocks) for better adaptation.
  SEM_SEG_HEAD:
    NUM_CLASSES: 5 
  MASK_FORMER:
    TEST:
      TASK: 'vis'  # video instance segmentation task
      WINDOW_INFERENCE: True  # run inference on small frame windows for memory savings
      WINDOW_SIZE: 15  # size of the window for inference (smaller = less memory)
  VIDEO_HEAD:
    NUM_NEW_INS: 10  # maximum number of new instances to consider per frame

DATASETS:
  DATASET_RATIO: [1.0] # Ratio for mixing multiple datasets (1.0 means use all of this dataset)
  DATASET_NEED_MAP: [False]  # whether the dataset needs category mapping (False = no mapping needed)
  DATASET_TYPE: ['video_instance']  # type of dataset (video_instance = video instance segmentation)
  TRAIN: ("ytvis_fishway_train",)  # training dataset
  TEST: ("ytvis_fishway_val",)  # testing dataset

SOLVER:
  IMS_PER_BATCH: 1  # Images per batch... Adjust based on your GPU memory
  BASE_LR: 0.0001  # base learning rate... keep low for fine tuning
  STEPS: (1656, 2208)     # Iterations at which the learning rate is decreased (60% and 80% of 2760)
  MAX_ITER: 2760     # Maximum number of training iterations (15 epochs - 184 vids)
  WARMUP_ITERS: 138   # Number of iterations for learning rate warmup (5% of 2760)
  CHECKPOINT_PERIOD: 184  # How often (in iterations) to save checkpoints (every epoch)
  BACKBONE_MULTIPLIER: 0.1  # Multiplier for the learning rate of the backbone
  # Memory optimization settings
  CLIP_GRADIENTS:
    ENABLED: True
    CLIP_TYPE: "norm"
    CLIP_VALUE: 0.1
    NORM_TYPE: 2.0

INPUT:
  # Enhanced size variations for better generalization
  MIN_SIZE_TRAIN: (320, 352, 392, 416, 448, 480, 512, 544, 576, 608, 640)
  MIN_SIZE_TRAIN_SAMPLING: "choice_by_clip"  # Sample different sizes per clip
  MAX_SIZE_TRAIN: 768  # Increased max size for better resolution
  
  # Testing sizes
  MIN_SIZE_TEST: 224 # shorter side of images is resized to this value during testing
  MAX_SIZE_TEST: 320 # longer side is capped at this value during testing
  
  # Frame sampling
  SAMPLING_FRAME_NUM: 15 # number of frames to sample from each video clip during training
  SAMPLING_FRAME_RANGE: 7 # how many frames to sample before and after the reference frame
  SAMPLING_FRAME_STRIDE: 1 # 1 is every frame, 2 is every other frame, etc.
  
  # MOTION-PRESERVING AUGMENTATIONS ONLY
  # REMOVED: REVERSE_AGU - This would destroy temporal motion patterns
  RANDOM_FLIP: "flip_by_clip"  # Flip all frames in a clip the same way
  FORMAT: "RGB"  # Color format of the images
  
  # Crop augmentation for better generalization
  CROP:
    ENABLED: True
    TYPE: "absolute_range"
    SIZE: (384, 600)
  
  # Large Scale Jittering for robust training
  LSJ_AUG:
    ENABLED: True
    IMAGE_SIZE: 768
    MIN_SCALE: 0.1
    MAX_SCALE: 2.0
  
  # Supported augmentations (motion-preserving)
  AUGMENTATIONS: ['brightness', 'contrast', 'saturation']  # Motion-preserving color augmentations
  # Note: 'rotation' removed as it would distort motion trajectories

DATALOADER:
  FILTER_EMPTY_ANNOTATIONS: True # whether to filter out images with no annotations

OUTPUT_DIR: '/store/simone/dvis-model-outputs/trained_models/base_model_unmasked'